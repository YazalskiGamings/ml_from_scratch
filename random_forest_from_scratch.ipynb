{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9706091,"sourceType":"datasetVersion","datasetId":5936188}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hattoriyoung/random-forest-from-scratch?scriptVersionId=221550550\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"#### **Random Forest From Scratch**","metadata":{}},{"cell_type":"markdown","source":"**Introduction**\n\nRandom Forest is an ensemble learning method for classification and regression. An ***ensemble learning*** method is it combines the predictions of multiple weak models to make a stronger, more reliable prediction. That way it is able to reduce errors and improve performance. Esemble Learning are also cateogirsed as \n* *Bagging*\n* *Boosting*\n\nRandom Forest uses the former.\n\nWhat is Random Forest? It essentially takes a collection of weaker decision trees that is trained by ***bootstraping*** (*sampling with replacement*) your data. This bootstrapping method is applied to both features and data, meaning your data can have repeats and your features will also have repeats. Afterwards, after training each of your individual decision trees, the trees will make the predictions individually and for classification, majority voting is implemented while regression, we take the average of all the results of the trees; This entire process is called ***Bagging***. \n\nThis algorithim is also non-parametric like the decision trees, however, different from the decision tree, the random forest algorithim are uncorrelated and solves the overfitting issue. \n\nThe algorithim has alot of relevance to the decision tree alogirthim, therefore, if you have any questions or unfamiliar with the decision trees, you can check out on my profile which I also implemented from scratch: [Decision Trees From Scratch](https://www.kaggle.com/code/hattoriyoung/dt-from-scratch)\n\nP.S Cool thing about random forest is that is able to find features that contribute to the reduction in impurity, which evaluates feature importantance. \n\n**Assumptions**\n\n* Each tree makes its own decisions: Every tree in the forest makes its own predictions without relying on others.\n* Random parts of the data are used: Each tree is built using random samples and features to reduce mistakes.\n* Enough data is needed: Sufficient data ensures the trees are different and learn unique patterns and variety.\n* Different predictions improve accuracy: Combining the predictions from different trees leads to a more accurate final results.\n\n**Goal**\n\nThe Goal of this excerise is to implement the alogrithim from scratch without using any frameworks.\n\n**Motivation**\n\nAs for the motivation, these kind of excerises are good for any data science interview preperation as you are able to get the fundementals down. Understand how the algorithim works and not treating it like a black box function will help you become a better data scientist.","metadata":{}},{"cell_type":"markdown","source":"-----","metadata":{}},{"cell_type":"markdown","source":"### **Python Implementation From Scratch**","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt # plot\n\nfrom collections import Counter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-02-08T21:40:25.86505Z","iopub.execute_input":"2025-02-08T21:40:25.865463Z","iopub.status.idle":"2025-02-08T21:40:27.300829Z","shell.execute_reply.started":"2025-02-08T21:40:25.865427Z","shell.execute_reply":"2025-02-08T21:40:27.299779Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mydf = pd.read_csv(\"/kaggle/input/mountains-vs-beaches-preference/mountains_vs_beaches_preferences.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-02-08T21:40:29.117612Z","iopub.execute_input":"2025-02-08T21:40:29.118092Z","iopub.status.idle":"2025-02-08T21:40:29.281919Z","shell.execute_reply.started":"2025-02-08T21:40:29.118057Z","shell.execute_reply":"2025-02-08T21:40:29.280959Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For simplicity, we will be choosing these following variables for our model creation. \n- `Age`: Age of the individual (numerical).\n- `Travel_Frequency`: Number of vacations taken per year \n- `Vacation_Budget`: The individual's budget when going on vacation.\n- `Pets`: Indicates whether the individual owns pets (binary: 0 = No, 1 = Yes).\n\nGiven the fact that decision trees are prone to overfitting, let's just keep our features to the minimum and simple. Random forest can be a good alternative in most cases when your data is more complex. \n\n*NOTE: Generally, having domain knowledge about your data is important to choose the right features if you have loads to feature to choose from. That being said, there are many use cases in using feature selection techniques to accomplish this.*","metadata":{}},{"cell_type":"code","source":"mydf = mydf[[\"Age\",\"Travel_Frequency\",\"Vacation_Budget\",\"Pets\",\"Preference\"]]\nmydf.head(n = 10)","metadata":{"execution":{"iopub.status.busy":"2025-02-08T23:55:46.939066Z","iopub.execute_input":"2025-02-08T23:55:46.93977Z","iopub.status.idle":"2025-02-08T23:55:46.957221Z","shell.execute_reply.started":"2025-02-08T23:55:46.939637Z","shell.execute_reply":"2025-02-08T23:55:46.955634Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"------------","metadata":{}},{"cell_type":"markdown","source":"### 1. **Random Forest Using Sklearn**\n\nSplit features and outcome variables as well as create 80/20 Train & Test set.\n\nImportant Hyperparameters to understand if you are using Random Forest alogirithim to solve your problem.\n\n- **n_estimators**: # of trees in your random forest. More trees mean better model performance.\n- **max_features**: # of features when splitting a node. This makes sure that the tree will not overfit as it controls how many features in a tree.\n- **max_depth**: This controls the depth of the tree, the deeper the tree is, the more it is prone to overfitting\n- **min_sample_split**: This controls the minimum # of samples to split per node.\n- **max_leaf_nodes**: Controls the # of leaf nodes in a tree, hence, controlling the model size & complexity.\n- **max_sample**: Controls how much data are given per tree based on our training set.\n\n","metadata":{}},{"cell_type":"code","source":"import random\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import tree\n\nfrom collections import Counter\nfrom scipy.stats import mode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T23:48:46.936222Z","iopub.execute_input":"2025-02-08T23:48:46.93662Z","iopub.status.idle":"2025-02-08T23:48:46.942938Z","shell.execute_reply.started":"2025-02-08T23:48:46.936588Z","shell.execute_reply":"2025-02-08T23:48:46.941487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_data = mydf[[\"Age\",\"Travel_Frequency\",\"Vacation_Budget\",\"Pets\"]].values\nY_target = mydf[\"Preference\"].values","metadata":{"execution":{"iopub.status.busy":"2025-02-08T21:40:37.022747Z","iopub.execute_input":"2025-02-08T21:40:37.023267Z","iopub.status.idle":"2025-02-08T21:40:37.029823Z","shell.execute_reply.started":"2025-02-08T21:40:37.023233Z","shell.execute_reply":"2025-02-08T21:40:37.028519Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_data,Y_target, random_state=42, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2025-02-08T21:40:39.939831Z","iopub.execute_input":"2025-02-08T21:40:39.940325Z","iopub.status.idle":"2025-02-08T21:40:39.953341Z","shell.execute_reply.started":"2025-02-08T21:40:39.940283Z","shell.execute_reply":"2025-02-08T21:40:39.9521Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def random_forest(estimator,depth,min_split):\n    model = RandomForestClassifier(n_estimators = estimator, max_depth = depth, min_samples_split = min_split,random_state = 42)\n    return model.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2025-02-08T21:40:41.143048Z","iopub.execute_input":"2025-02-08T21:40:41.143511Z","iopub.status.idle":"2025-02-08T21:40:41.151745Z","shell.execute_reply.started":"2025-02-08T21:40:41.143474Z","shell.execute_reply":"2025-02-08T21:40:41.148862Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prediction(X_test,clf):\n    y_pred = clf.predict(X_test)\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2025-02-08T21:40:42.61011Z","iopub.execute_input":"2025-02-08T21:40:42.610546Z","iopub.status.idle":"2025-02-08T21:40:42.616116Z","shell.execute_reply.started":"2025-02-08T21:40:42.61051Z","shell.execute_reply":"2025-02-08T21:40:42.614674Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def accuracy(y_test, pred):\n    return accuracy_score(y_test,pred)","metadata":{"execution":{"iopub.status.busy":"2025-02-08T21:40:43.611235Z","iopub.execute_input":"2025-02-08T21:40:43.611634Z","iopub.status.idle":"2025-02-08T21:40:43.617468Z","shell.execute_reply.started":"2025-02-08T21:40:43.611598Z","shell.execute_reply":"2025-02-08T21:40:43.61616Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train\nrf_model = random_forest(estimator = 200, depth = 15, min_split = 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T23:57:08.667956Z","iopub.execute_input":"2025-02-08T23:57:08.668431Z","iopub.status.idle":"2025-02-08T23:57:18.082215Z","shell.execute_reply.started":"2025-02-08T23:57:08.668394Z","shell.execute_reply":"2025-02-08T23:57:18.081246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict\npred_rf = prediction(X_test,rf_model)","metadata":{"execution":{"iopub.status.busy":"2025-02-08T21:41:02.611947Z","iopub.execute_input":"2025-02-08T21:41:02.612378Z","iopub.status.idle":"2025-02-08T21:41:02.963333Z","shell.execute_reply.started":"2025-02-08T21:41:02.612342Z","shell.execute_reply":"2025-02-08T21:41:02.962424Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result = accuracy(y_test, pred_rf)\nclassification_rep = classification_report(y_test, pred_rf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T21:41:05.025305Z","iopub.execute_input":"2025-02-08T21:41:05.025695Z","iopub.status.idle":"2025-02-08T21:41:05.05861Z","shell.execute_reply.started":"2025-02-08T21:41:05.025662Z","shell.execute_reply":"2025-02-08T21:41:05.057501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Accuracy: {round(result,4) * 100}%\")\nprint(\"\\nClassification Report:\\n\", classification_rep)","metadata":{"execution":{"iopub.status.busy":"2025-02-08T21:41:07.041532Z","iopub.execute_input":"2025-02-08T21:41:07.041941Z","iopub.status.idle":"2025-02-08T21:41:07.048657Z","shell.execute_reply.started":"2025-02-08T21:41:07.041906Z","shell.execute_reply":"2025-02-08T21:41:07.047486Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"----------------","metadata":{}},{"cell_type":"markdown","source":"### 2. Random Forest From Scratch\n\n#### **Gini vs Entropy AND Information Gain**\n\n* **Gini**: Measures the likelihood of an incorrect classification of a new instance if it was randomly classified according to the distribution of classes in the dataset.\n    * Defined as $G = 1 - \\Sigma p^2_j$, where $p_j$ is the probability of class j.\n\n<br>\n\n* **Entropy**: Measures the amount of uncertainty or impurity in the dataset.\n    * Defined as $H(X) = -\\Sigma p_j*log(p_j)$, where $p_j$ is the probability of class j.\n\n<br>\n\n* **Information Gain (IG)**: Measures the reduction in entropy or Gini impurity after a dataset is split on an attribute.\n    * Defined as $IG(D) = I(D_p) - (\\frac{N_{left}}{N_p} * I(D_{left}) - \\frac{N_{right}}{N_p} * I(D_{right}))$, where $D_p,D_{left},D_{right}$ represent the datasets from the parent, left, and right children nodes, $N_p, N_{left}, N_{right}$ represent the number of observations in the parent, left and right children nodes and $I(D)$ denotes the entropy for that particular node.\n    * Essentially, taking the difference between before the split vs after the split and determine if there is any information gain, and repeat this for each split of the feature/node.","metadata":{}},{"cell_type":"code","source":"class DecisionTreeFromScratch:\n    def __init__(self,min_sample,max_depth,max_splits):\n        self.tree = None # store tree\n        self.min_sample = min_sample # min # to split node\n        self.max_depth = max_depth # max depth of tree\n        self.max_splits = max_splits # control complexity of tree\n\n    def _entropy(self,y):\n        label_counts = Counter(y) # count # of labels per outcome\n        entropy = 0.0\n\n        for counts in label_counts.values():\n            prob = counts / len(y)\n            entropy -= prob * np.log2(prob)\n\n        return entropy # return scalar\n\n    def _info_gain(self,parent,left_child,right_child): # calc info gain after split\n        weight_l = len(left_child) / len(parent) \n        weight_r = len(right_child) / len(parent)\n        gain = self._entropy(parent) - (weight_l * self._entropy(left_child) + weight_r * self._entropy(right_child))\n\n        return gain # was the info gained from reduc entropy worth the split \n\n    def _best_split(self,X,y,n_features):\n        best_split = {\"info_gain\": 0}\n\n        for feature in range(n_features):\n            feature_values = X[:,feature]\n            unique_feature_values = np.unique(feature_values)\n\n            if len(unique_feature_values) > self.max_splits: # control # of splits per feature\n                unique_feature_values = np.linspace(min(feature_values),max(feature_values),self.max_splits) \n\n            for threshold in unique_feature_values: # for each feature, split left & right and iterate each threshold and calculate/compare info_gain\n                left_index = np.where(feature_values <= threshold)\n                right_index = np.where(feature_values > threshold)\n\n                if len(left_index) == 0 or len(right_index) == 0:\n                    continue\n\n                y_left,y_right = y[left_index], y[right_index]\n                gain = self._info_gain(y,y_left,y_right)\n\n                if gain > best_split[\"info_gain\"]: # only save the best split per feature that maximizes info_gain\n                    best_split = {\n                        \"feature_index\": feature_index,\n                        \"threshold\": threshold,\n                        \"left_index\": left_index,\n                        \"right_index\": right_index,\n                        \"information_gain\": gain\n                    }\n\n        return best_split\n\n    def common_label(self,y):\n        return Counter(y).most_common(1)[0][0]\n\n    def _build_tree(self,X,y, depth = 0):\n        n,n_features = X.shape\n        n_labels = len(np.unique(y))\n\n        if n_labels == 1 or n < self.min_sample or depth < self.max_depth: # base case\n            return self.common_label(y) # return most common label\n\n        best_split = self._best_split(X,y,n_features) # calc best split\n        \n        if best_split[\"info_gain\"] == 0:\n            return self.common_label(y)\n        # recursively build left & right subtrees finding best_split until base case is met\n        left_subtree = self._build_tree(X[best_split[\"left_index\"]], y[best_split[\"left_index\"]], depth + 1)\n        right_subtree = self._build_tree(X[best_split[\"right_index\"]], y[best_split[\"right_index\"]], depth + 1)\n\n        return {\n            \"feature_index\": best_split[\"feature_index\"],\n            \"threshold\": best_split[\"threshold\"],\n            \"left_tree\": left_subtree,\n            \"right_tree\": right_subtree\n        }\n\n    def fit(self,X,y):\n        self.tree = self._build_tree(X,y) \n    \n    def _predict_single(self,x,tree):\n        if not isinstance(tree,dict): # base case: is it a leaf node?\n            return tree\n        \n        feature_index = tree[\"feature_index\"]\n        threshold = tree[\"threshold\"]\n        \n        if x[feature_index] <= threshold: # the vals for that data, do we traverse left or right?\n            return self._predict_single(x, tree[\"left_tree\"]) # traverse left, if <= threshold until base case\n        else:\n            return self._predict_single(x, tree[\"right_tree\"]) # traverse right, if > threshold until base case\n        \n    def predict(self,X):\n        return np.array([self._predict_single(x,self.tree) for x in X]) # wrapper for _predict_single, repeat x times","metadata":{"execution":{"iopub.status.busy":"2025-02-08T21:41:14.938431Z","iopub.execute_input":"2025-02-08T21:41:14.938977Z","iopub.status.idle":"2025-02-08T21:41:14.95855Z","shell.execute_reply.started":"2025-02-08T21:41:14.938857Z","shell.execute_reply":"2025-02-08T21:41:14.957126Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RandomForestFromScratch:\n    def __init__(self, n_estimators=10, min_sample=1, max_depth=10, max_splits=20):\n        self.n_estimators = n_estimators\n        self.min_sample = min_sample\n        self.max_depth = max_depth\n        self.max_splits = max_splits\n        self.trees = []\n\n    def bootstrap(self, X, y):\n        # Bootstrap sample with replacement\n        n_samples = X.shape[0]\n        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        return X[indices], y[indices]\n\n    def fit(self, X, y):\n        self.trees = []\n        for _ in range(self.n_estimators): \n            X_sample, y_sample = self.bootstrap(X, y)\n            tree = DecisionTreeFromScratch(min_sample=self.min_sample, \n                                           max_depth=self.max_depth, \n                                           max_splits=self.max_splits)\n            tree.fit(X_sample, y_sample)\n            self.trees.append(tree)\n\n    def predict(self, X):\n        preds = [tree.predict(X) for tree in self.trees]\n        preds = np.array(preds)\n        if preds.ndim == 2 and preds.shape[0] > 1: # make sure there are more > 1 tree within rf.\n            preds = mode(preds, axis=0).mode[0] # ensemble voting\n        return preds\n\n    def score(self, X, y):\n        preds = self.predict(X)\n        return np.mean(preds == y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T23:03:49.852416Z","iopub.execute_input":"2025-02-08T23:03:49.852803Z","iopub.status.idle":"2025-02-08T23:03:49.863631Z","shell.execute_reply.started":"2025-02-08T23:03:49.852771Z","shell.execute_reply":"2025-02-08T23:03:49.862463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf_model_scratch = RandomForestFromScratch(n_estimators=200, min_sample=2, max_depth=10, max_splits=15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T23:03:51.657594Z","iopub.execute_input":"2025-02-08T23:03:51.657984Z","iopub.status.idle":"2025-02-08T23:03:51.662941Z","shell.execute_reply.started":"2025-02-08T23:03:51.657949Z","shell.execute_reply":"2025-02-08T23:03:51.661798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf_model_scratch.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T23:03:53.546406Z","iopub.execute_input":"2025-02-08T23:03:53.547364Z","iopub.status.idle":"2025-02-08T23:03:55.770504Z","shell.execute_reply.started":"2025-02-08T23:03:53.547295Z","shell.execute_reply":"2025-02-08T23:03:55.769301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = rf_model_scratch.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2025-02-08T23:03:56.438273Z","iopub.execute_input":"2025-02-08T23:03:56.438689Z","iopub.status.idle":"2025-02-08T23:03:57.181608Z","shell.execute_reply.started":"2025-02-08T23:03:56.438655Z","shell.execute_reply":"2025-02-08T23:03:57.180258Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"accuracy = rf_model_scratch.score(X_test, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T23:03:58.190872Z","iopub.execute_input":"2025-02-08T23:03:58.191287Z","iopub.status.idle":"2025-02-08T23:03:58.93612Z","shell.execute_reply.started":"2025-02-08T23:03:58.191252Z","shell.execute_reply":"2025-02-08T23:03:58.934914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Accuracy: {round(accuracy * 100,1)}%\")","metadata":{"execution":{"iopub.status.busy":"2025-02-08T23:04:35.810813Z","iopub.execute_input":"2025-02-08T23:04:35.811216Z","iopub.status.idle":"2025-02-08T23:04:35.81749Z","shell.execute_reply.started":"2025-02-08T23:04:35.811172Z","shell.execute_reply":"2025-02-08T23:04:35.816356Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---------","metadata":{}},{"cell_type":"markdown","source":"### **Feature Importance**\n\nTaking a look at Feature Importance on a high level","metadata":{}},{"cell_type":"code","source":"feature_importance = rf_model.feature_importances_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_names = mydf.columns[0:4]\nfeature_importance_df = pd.DataFrame({\"Feature\": feature_names,\"Gini Importance\":feature_importance}).sort_values(\"Gini Importance\",ascending = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nplt.barh(feature_names, feature_importance, color='red')\nplt.xlabel('Gini Importance')\nplt.title('Feature Importance - Gini Importance')\nplt.gca().invert_yaxis()  # Invert y-axis for better visualization\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Gini importance utulization during feature importance is calculated by the total reduction in impurity for that feature, therefore, we can observe that `vacation_Budget` contributed the most in decreasing the impurity, as a result, given the most weight in Gini importance among multiple trees. ","metadata":{}},{"cell_type":"markdown","source":"-------","metadata":{}},{"cell_type":"markdown","source":"### **Results**\n\nBoth results are adequate resulting a ~ 75% accuracy. \n\nThe most important thing about random forest is to be able tune the hyperparameters of the random forest. Throughout this excerise, there was alot of trial & errors in trying to make sure the Random Forest will output the most optimized results. Ofcourse, both GridSearchCV and RandomizedSearchCV can be called as part of sklearn.model_selection, however, I decided to brute force it.\n\nThe model building from scratch was not a diffcult task, as it was eseentially calling n_estimators of decision tree and having features and observations being bootstrapped. ","metadata":{}}]}